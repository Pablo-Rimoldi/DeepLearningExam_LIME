{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/drive/1KSksZPsW6rNQfGW69wS8j5ietj3ayUkC\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coQ7nfcyPZTN"
      },
      "source": [
        "Notes :\n",
        "- This notebook takes around 6 minutes to run.\n",
        "- I underlined the differences between the exam in the following code like this: <font color=\"red\">**CHANGE**</font>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJOYXIX2foS0"
      },
      "source": [
        "# 0 - Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1NRefM8VtU3"
      },
      "source": [
        "This section introduces the dataset loading process, utilizing the requests library to download the necessary data from the GitHub repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDWq8b-9b8us",
        "outputId": "7e0e619b-94bc-4d96-8208-c0157221be63"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "url = \"https://github.com/PaRi29/DeepLearningExam/raw/main/assets/input_data.pkl\"\n",
        "response = requests.get(url)\n",
        "with open(\"input_data.pkl\", \"wb\") as f:\n",
        "    f.write(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2FxJHYqoQjze"
      },
      "outputs": [],
      "source": [
        "%pip install tensorflow\n",
        "\n",
        "import warnings\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import string\n",
        "import unicodedata\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import itertools\n",
        "import keras\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import accuracy_score, f1_score, mean_squared_error, confusion_matrix\n",
        "\n",
        "from keras.regularizers import l1, l2\n",
        "from keras.layers import (\n",
        "    Input, Dense, BatchNormalization, Dropout, Embedding,\n",
        "    Bidirectional, LSTM, Concatenate\n",
        ")\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I suppress warnings to keep the output clean; mainly related to known deprecations in libraries that do not impact the result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "sanity check, everything is working fine "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_pickle(\"input_data.pkl\")\n",
        "df.drop(columns=[\"Average_Score\"],inplace=True) #Average_Score ws not part of the exam\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To adhere to the practical time constraints of this evaluation, I have implemented a two-fold strategy to manage the overall computational cost while maintaining the methodological integrity of the project. This involves a trade-off between the volume of data processed and the breadth of hyperparameter exploration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_full = df.copy()\n",
        "\n",
        "sample_fraction = 0.50\n",
        "target_column = 'Review_Type'\n",
        "reduced_df, _ = train_test_split(\n",
        "    df_full,\n",
        "    train_size=sample_fraction,\n",
        "    stratify=df_full[target_column],\n",
        "    random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEZTewjCJMhr"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUAV0PpkfrJD"
      },
      "source": [
        "# 2 - Input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0-cYhGUVnRx"
      },
      "source": [
        "This section in the exam was divided in two subparts:\n",
        "* How to (if) preprocess input data and which data would you retain/use;\n",
        "* Which is the input of the model, and how is it represented;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "to avoid any kind of data leakage, I first divide into train, validation, and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df, temp_df = train_test_split(\n",
        "    reduced_df, test_size=0.20, random_state=42, shuffle=True\n",
        ")\n",
        "val_df, test_df = train_test_split(\n",
        "    temp_df, test_size=0.50, random_state=42, shuffle=True\n",
        ")\n",
        "print(len(train_df), len(val_df), len(test_df))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.1 preprocessing:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2JHMOxAHe_1"
      },
      "source": [
        "### 2.1.1 - Hotel_Name\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNLJvK6uf4Zd"
      },
      "source": [
        "In my exam solution, I stated that I would employ **one‑hot encoding** for the hotel name. I consider this feature important for the following reasons:\n",
        "1. **Unique index**  \n",
        "   One‑hot encoding transforms each hotel into a distinct binary vector that acts as a unique identifier.  \n",
        "   - This allows the model to “recognize” and link reviews belonging to the same hotel, preserving continuity with previously seen reviews of that property.\n",
        "2. **Preservation of hotel‑specific patterns**  \n",
        "   By providing a sparse, binary representation, the model can learn hotel‑specific tendencies (for example, whether a given hotel generally receives more positive or negative feedback).\n",
        "3. **Technical feasibility**  \n",
        "   This approach remains practical **only if** the number of unique hotels stays within a manageable range. <font color=\"red\">Small **CHANGE**</font> To control dimensionality, I group all hotels with fewer than a chosen cutoff of reviews (e.g., ≥ 20 or ≥ 25) into an “Other” category, limiting the total number of one-hot columns to a few hundred instead of thousands. After analyzing the impact of different cutoff thresholds on dimensionality, I chose min_review_cutoff = 20. This value represents a good compromise: it reduces the dimensions from over 1100 to a manageable ~31, while preserving the information on the most reviewed hotels that make up a significant part of the dataset. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hotel_name_counts = train_df['Hotel_Name'].value_counts()\n",
        "print(hotel_name_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for cutoff in [3, 5, 10, 20,25,30,35,40,45, 50]:\n",
        "    n_slots = (hotel_name_counts >= cutoff).sum() + 1  \n",
        "    print(f\"min_reviews ≥{cutoff:2d} ==> {n_slots:3d} one-hot dimensions\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hotel_counts_train = train_df['Hotel_Name'].value_counts()\n",
        "min_review_cutoff = 20\n",
        "\n",
        "frequent_hotels   = hotel_counts_train[hotel_counts_train >= min_review_cutoff].index\n",
        "\n",
        "def encode_hotel(df, frequent_hotels):\n",
        "    s = df['Hotel_Name'].where(df['Hotel_Name'].isin(frequent_hotels), 'Other')\n",
        "    return pd.get_dummies(s, prefix='hotel')\n",
        "\n",
        "train_hot = encode_hotel(train_df, frequent_hotels)\n",
        "val_hot   = encode_hotel(val_df,   frequent_hotels)\n",
        "test_hot  = encode_hotel(test_df,  frequent_hotels)\n",
        "col_train = train_hot.columns\n",
        "val_hot   = val_hot.reindex(columns=col_train, fill_value=0)\n",
        "test_hot  = test_hot.reindex(columns=col_train, fill_value=0)\n",
        "\n",
        "\n",
        "train_hot = train_hot.astype(np.float32)\n",
        "val_hot = val_hot.astype(np.float32)\n",
        "test_hot = test_hot.astype(np.float32)\n",
        "print(\"Shape one‑hot:\", train_hot.shape, val_hot.shape, test_hot.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vr_NF9M0gjHM"
      },
      "source": [
        "### 2.1.2 - Hotel_Address\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bs-FwoFCcO_9"
      },
      "source": [
        "As stated in the exam, I decided to drop this information because I think it could not bring strong valuable insights "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cols_to_drop = [\"Hotel_Address\"]\n",
        "train_df.drop(columns=cols_to_drop, inplace=True)\n",
        "val_df.drop(columns=cols_to_drop, inplace=True)\n",
        "test_df.drop(columns=cols_to_drop, inplace=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBd6q82WaaxU"
      },
      "source": [
        "### 2.1.3 - Review Date"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As stated in the exam, I am going to:\n",
        "\n",
        "1. drop the day\n",
        "2. convert the month using a cyclic sine/cosine function\n",
        "3. convert the year, <font color=\"red\">small **CHANGE**</font>: The conversion of the year into the number of years *since today* is partially incorrect because it makes the feature dependent on the training/deployment date, reducing model consistency. Instead, I convert the year into the number of years *since the earliest* review date in the dataset, ensuring stable and meaningful temporal representation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IibP1Z1xX4f",
        "outputId": "8cf5045a-81d2-4938-b284-5d78400416dc"
      },
      "outputs": [],
      "source": [
        "def transform_dates(df, min_year=None):\n",
        "    df['Review_Date'] = pd.to_datetime(df['Review_Date'], format='%m/%d/%Y')\n",
        "    df['Review_Month'] = df['Review_Date'].dt.month\n",
        "    df['Review_Year'] = df['Review_Date'].dt.year\n",
        "\n",
        "    df['Review_Month_sin'] = np.sin(2 * np.pi * df['Review_Month'] / 12)\n",
        "    df['Review_Month_cos'] = np.cos(2 * np.pi * df['Review_Month'] / 12)\n",
        "\n",
        "    if min_year is None:\n",
        "        min_year = df['Review_Year'].min()\n",
        "    df['Review_Year_Since'] = df['Review_Year'] - min_year\n",
        "\n",
        "    df.drop(columns=['Review_Date', 'Review_Month', 'Review_Year'], inplace=True)\n",
        "    return df, min_year\n",
        "\n",
        "train_df, min_year = transform_dates(train_df)\n",
        "val_df, _ = transform_dates(val_df, min_year)\n",
        "test_df, _ = transform_dates(test_df, min_year)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1.4 - Reviewer_Nationality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I chose to **exclude the \"Reviewer Nationality\"** feature due to concerns about **representation bias** and the risk of **spurious correlations**. While nationality might weakly correlate with review patterns, this is **not causally meaningful** and may reflect systemic imbalances — e.g., wealthier countries being overrepresented or visiting different types of hotels. Including it could lead the model to learn **stereotypical associations**, undermining fairness\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cols_to_drop = [\"Reviewer_Nationality\"]\n",
        "train_df.drop(columns=cols_to_drop, inplace=True)\n",
        "val_df.drop(columns=cols_to_drop, inplace=True)\n",
        "test_df.drop(columns=cols_to_drop, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1.5 Review text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I clean the review text by removing punctuation, stop words, and non-ASCII characters, and converting everything to lowercase\n",
        "\n",
        "<font color=\"red\">**CHANGE**</font> Using the term \"Bag of Words\" was imprecise. I didn't actually use the traditional BoW approach based on frequency vectors, but rather a structured representation feeding into an embedding layer. My intention was to express the idea of a structured input derived from the vocabulary. This terminological clarification does not modify the proposed architectural approach, but corrects the technical terminology for greater precision."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "def preprocess_review(text):\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')    \n",
        "    text = text.replace('--', ' ')    \n",
        "    words = text.split()    \n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    words = [w.translate(table) for w in words]    \n",
        "    words = [w.lower() for w in words if w.isalpha() and w.lower() not in stop_words]\n",
        "    return ' '.join(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To better understand the input data, I analyze the distribution of the number of words in each review after preprocessing. I compute basic statistics and plot a histogram to visualize how review lengths are spread. This helps verify whether the maximum length of 400 words is appropriate, or if a shorter cutoff would be sufficient for most reviews."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for df_ in [train_df, val_df, test_df]:\n",
        "    df_[\"clean_text\"] = df_[\"Review\"].apply(preprocess_review)\n",
        "    df_[\"review_length\"] = df_[\"clean_text\"].apply(lambda x: len(x.split()))\n",
        "    df_[\"tokens\"] = df_[\"clean_text\"].str.split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "review_length_stats = train_df[\"review_length\"].describe()\n",
        "print(review_length_stats)\n",
        "print(\"\\n\")\n",
        "\n",
        "quantiles = [0.50, 0.75, 0.90, 0.95, 0.98, 0.99, 1.00]\n",
        "for q in quantiles:\n",
        "    length_at_quantile = train_df[\"review_length\"].quantile(q)\n",
        "    print(f\"Quantile {q*100:.0f}%: Reviews have length <= {length_at_quantile:.0f} words.\")\n",
        "\n",
        "plt.figure(figsize=(14, 7))\n",
        "sns.histplot(data=train_df, x=\"review_length\", bins=50, kde=True,\n",
        "             label=\"Length distribution\", stat=\"density\")\n",
        "plt.axvline(train_df[\"review_length\"].quantile(0.75), color='green', linestyle='--',\n",
        "            label=f'75th Quantile ({train_df[\"review_length\"].quantile(0.75):.0f} words)')\n",
        "plt.axvline(train_df[\"review_length\"].quantile(0.90), color='orange', linestyle='--',\n",
        "            label=f'90th Quantile ({train_df[\"review_length\"].quantile(0.90):.0f} words)')\n",
        "plt.axvline(train_df[\"review_length\"].quantile(0.95), color='red', linestyle='--',\n",
        "            label=f'95th Quantile ({train_df[\"review_length\"].quantile(0.95):.0f} words)')\n",
        "plt.title('Distribution of Review Lengths with Highlighted Quantiles', fontsize=16)\n",
        "plt.xlabel('Number of Words per Review', fontsize=12)\n",
        "plt.ylabel('Density', fontsize=12)\n",
        "plt.legend()\n",
        "plt.xlim(0, train_df[\"review_length\"].quantile(0.99))\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Based on the text preprocessing, a detailed analysis of the review lengths was conducted. The statistical summary and quantile distribution reveal several key insights:\n",
        "\n",
        "*   The reviews are predominantly short, with a **mean length of 14 words** and a **median of only 6 words**.\n",
        "*   The distribution is heavily skewed, as confirmed by the quantiles: **75% of reviews have 16 words or fewer**.\n",
        "*   **95% of all reviews are shorter than 50 words**, and **99% are shorter than 102 words**. The absolute maximum length found in the dataset is 274 words.\n",
        "\n",
        "<font color=\"red\">**CHANGE**</font> In the exam, I said that without an analysis of the distribution of words (that I have just done above), I would have kept 400 as the upper bound. However, based on the review length distribution, where 75% of reviews have 16 words or fewer and the maximum length is 274 words, I decide to **truncate sequences at 100 words**. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_train_words = [word for tokens in train_df['tokens'] for word in tokens]\n",
        "unique_words = sorted(list(set(all_train_words)))\n",
        "\n",
        "word_index = {word: i + 2 for i, word in enumerate(unique_words)}\n",
        "word_index[\"<OOV>\"] = 1\n",
        "MAX_LEN = 100\n",
        "VOCAB_SIZE = len(unique_words) + 2\n",
        "\n",
        "\n",
        "def text_to_sequence(tokens, word_index_dict):\n",
        "    seq = [word_index_dict.get(word, word_index_dict[\"<OOV>\"]) for word in tokens]\n",
        "    return seq\n",
        "\n",
        "train_sequences = [text_to_sequence(tokens, word_index) for tokens in train_df['tokens']]\n",
        "val_sequences = [text_to_sequence(tokens, word_index) for tokens in val_df['tokens']]\n",
        "test_sequences = [text_to_sequence(tokens, word_index) for tokens in test_df['tokens']]\n",
        "\n",
        "train_seqs = pad_sequences(train_sequences, maxlen=MAX_LEN, padding='post', truncating='post')\n",
        "val_seqs = pad_sequences(val_sequences, maxlen=MAX_LEN, padding='post', truncating='post')\n",
        "test_seqs = pad_sequences(test_sequences, maxlen=MAX_LEN, padding='post', truncating='post')\n",
        "\n",
        "print(\"Shape of final training sequences tensor:\", train_seqs.shape)\n",
        "print(\"Example of a processed sequence:\", train_seqs[27])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1.6 - Hotel number reviews/Reviewer number reviews, "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scaler = MinMaxScaler()\n",
        "numeric_cols = ['Hotel_number_reviews', 'Reviewer_number_reviews', 'Review_Year_Since', \n",
        "                'Review_Month_sin', 'Review_Month_cos']\n",
        "\n",
        "train_nums = scaler.fit_transform(train_df[numeric_cols])\n",
        "val_nums   = scaler.transform(val_df[numeric_cols])\n",
        "test_nums  = scaler.transform(test_df[numeric_cols])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjICOJlGQAIz"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.1. Input of the model "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The model architecture is designed to accept three distinct and parallel inputs, each representing a different type of information extracted from the original data. This multi-input architecture allows the model to learn specific representations for each data type before merging them for the final prediction.\n",
        "\n",
        "The three input tensors are summarized below, showing the shape and a brief description of their content. These tensors represent the textual, categorical (hotel), and numerical data, respectively.\n",
        "\n",
        "As outlined in the exam, the textual data input will be processed by the Bidirectional LSTM layer. The other two inputs (categorical and numerical), which are here separated just for clarity, will be concatenated with the output representation from the Bi-LSTM. This combined vector will then serve as the input for the subsequent fully-connected deep neural network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "print(\"\\n1. TEXTUAL INPUT (Padded Token Sequences)\")\n",
        "print(f\"   - Training tensor shape: {train_seqs.shape}\")\n",
        "print(f\"   - Validation tensor shape: {val_seqs.shape}\")\n",
        "print(f\"   - Test tensor shape: {test_seqs.shape}\")\n",
        "\n",
        "print(\"\\n2. CATEGORICAL INPUT (One-Hot Encoded Hotels)\")\n",
        "print(f\"   - Training tensor shape: {train_hot.shape}\")\n",
        "print(f\"   - Validation tensor shape: {val_hot.shape}\")\n",
        "print(f\"   - Test tensor shape: {test_hot.shape}\")\n",
        "\n",
        "print(\"\\n3. NUMERICAL INPUT (Normalized Features)\")\n",
        "print(f\"   - Training tensor shape: {train_nums.shape}\")\n",
        "print(f\"   - Validation tensor shape: {val_nums.shape}\")\n",
        "print(f\"   - Test tensor shape: {test_nums.shape}\")\n",
        "print(f\"   - Included columns: {numeric_cols}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2yUKMrkaWn1"
      },
      "source": [
        "# 3 - 4 - 5 OUTPUT - LOSS - MODEL CONFIGURATION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOw9bozw7a-L"
      },
      "source": [
        "This section represents the following three parts:\n",
        "\n",
        "3. OUTPUT: How would you design the output layer and why;\n",
        "4. LOSS: Which loss function would you use to train your model and\n",
        "why;\n",
        "5. MODEL CONFIGURATION\n",
        "\n",
        "  * Model composition: composition of layers, regardless their number,\n",
        "  or their dimension, which can be object of tuning\n",
        "  * Which activation functions would you use;\n",
        "  * How (if) would you regularize/initialize your model\n",
        "  * On which hyperparameters would you perform the model selection (just list them)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3 Output:\n",
        "I need to map the Review_Type output as an integer: 0 = bad review, 1 = good review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "review_type_map = {'Bad_review': 0, 'Good_review': 1}\n",
        "\n",
        "train_df['Review_Type'] = train_df['Review_Type'].map(review_type_map)\n",
        "val_df['Review_Type']   = val_df['Review_Type'].map(review_type_map)\n",
        "test_df['Review_Type']  = test_df['Review_Type'].map(review_type_map)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4/5 LOSS - MODEL CONFIGURATION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lV2O53ZfLOCd"
      },
      "source": [
        "<font color=\"red\">**CHANGE**</font>.  I wrote in the exam that the output layer will be composed of two neurons, one predicting the review score (regression) and the other predicting the review type (binary classification). However, the implementation differs slightly as I use two separate output heads rather than a single layer with two neurons. This architectural choice provides better separation of concerns and allows for independent optimization of each task through dedicated loss functions and activation functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILhjDBpq5k5S"
      },
      "outputs": [],
      "source": [
        "\n",
        "def create_model(\n",
        "    text_vocab_size,\n",
        "    hotel_onehot_dim,\n",
        "    numeric_features,\n",
        "    text_max_len=100,\n",
        "    embedding_dim=128,\n",
        "    lstm_units=64,\n",
        "    hidden_layer_sizes=[128, 64, 32],\n",
        "    dropout_rate=0.1,\n",
        "    weight_reg=l2(0.01)\n",
        "):\n",
        "    text_input = Input(shape=(text_max_len,), name='text_input')\n",
        "    hotel_input = Input(shape=(hotel_onehot_dim,), name='hotel_input')\n",
        "    numeric_input = Input(shape=(numeric_features,), name='numeric_input')\n",
        "\n",
        "    x_text = Embedding(input_dim=text_vocab_size,\n",
        "                       output_dim=embedding_dim,\n",
        "                       input_length=text_max_len)(text_input)\n",
        "    x_text = Bidirectional(LSTM(lstm_units,\n",
        "                                activation='tanh',\n",
        "                                recurrent_activation='sigmoid',\n",
        "                                dropout=dropout_rate,\n",
        "                                recurrent_dropout=dropout_rate,\n",
        "                                kernel_initializer='glorot_uniform', \n",
        "                                recurrent_initializer='glorot_uniform'\n",
        "                               ))(x_text)\n",
        "\n",
        "    x = Concatenate()([x_text, hotel_input, numeric_input])\n",
        "\n",
        "    for i, size in enumerate(hidden_layer_sizes, start=1):\n",
        "        x = Dense(size,\n",
        "                  activation='relu',\n",
        "                  kernel_initializer='he_uniform',\n",
        "                  kernel_regularizer=weight_reg,\n",
        "                  name=f'dense_{i}')(x)\n",
        "        x = BatchNormalization(name=f'bn_{i}')(x)\n",
        "        x = Dropout(dropout_rate, name=f'dropout_{i}')(x)\n",
        "\n",
        "    score_output = Dense(1,\n",
        "                         activation='linear',\n",
        "                         kernel_regularizer=weight_reg,\n",
        "                         name='review_score')(x)\n",
        "    type_output = Dense(1,\n",
        "                        activation='sigmoid',\n",
        "                        kernel_regularizer=weight_reg,\n",
        "                        name='review_type')(x)\n",
        "\n",
        "    return Model(inputs=[text_input, hotel_input, numeric_input],\n",
        "                 outputs=[score_output, type_output])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<font color=\"red\">**CHANGE**</font>  I wrote in the exam that *\"For the sigmoid/tanh of the LSTM cell I'll use He-initialization, while for sigmoid/tanh i'll use Glorot/xavier\"*. This statement contains a repetition error where I mistakenly wrote \"sigmoid/tanh\" twice instead of distinguishing between different layer types. What I meant was: **He initialization for ReLU activations** in the dense layers and **Glorot/Xavier initialization for sigmoid/tanh activations**. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5 - Hyperparametr optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "param_grid = {\n",
        "    'learning_rate':     [2e-3, 5e-3, 1e-2],\n",
        "    'batch_size':        [64, 128, 256],\n",
        "    'epochs':            [5, 10],\n",
        "    'dropout_rate':      [0.1, 0.2, 0.3],\n",
        "    'weight_reg':        [l2(1e-4), l2(1e-3), l2(1e-2)],\n",
        "    'embedding_dim':     [32, 64, 128],\n",
        "    'lstm_units':        [32, 64, 128],\n",
        "    'optimizer':         ['adam'],\n",
        "    'hidden_layer_sizes': [\n",
        "        [128, 64],\n",
        "        [64, 32]\n",
        "    ],\n",
        "    'loss_weight_score': [2, 3, 5, 10],\n",
        "    'loss_weight_type':  [0.1, 0.3, 0.5]\n",
        "}\n",
        "num_samples = 3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- We are tuning parameters like learning_rate, dropout_rate, and loss_weight_score, which have a significant influence on model convergence and generalization without drastically increasing training time per trial. More computationally expensive parameters, such as the number of LSTM layers or a much wider range for embedding_dim, have been intentionally kept fixed or limited. This focused approach allows for an efficient yet meaningful optimization process within the available computational budget.\n",
        "\n",
        "- Due to time constraints, the following random search is limited to num_samples = 3. This serves as a proof of concept for the tuning methodology rather than an exhaustive search for an optimal model. For the final evaluation, a pre-vetted, well performing configuration is used to ensure a meaningful analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def random_search_multitask(param_grid, samples=num_samples):\n",
        "    combos = list(itertools.product(*param_grid.values()))\n",
        "    sampled = random.sample(combos, samples)\n",
        "    configs = [dict(zip(param_grid.keys(), c)) for c in sampled]\n",
        "\n",
        "    best = {\n",
        "        'overall':       (0, None),\n",
        "        'classification': (0, None),\n",
        "        'regression':    (np.inf, None)\n",
        "    }\n",
        "\n",
        "    for idx, cfg in enumerate(configs):\n",
        "        print(f\"\\n=== Training config {idx+1}/{len(configs)} ===\")\n",
        "        print(\"Config:\", cfg)\n",
        "        model = create_model(\n",
        "            text_vocab_size=VOCAB_SIZE,\n",
        "            hotel_onehot_dim=train_hot.shape[1],\n",
        "            numeric_features=train_nums.shape[1],\n",
        "            embedding_dim=cfg['embedding_dim'],\n",
        "            lstm_units=cfg['lstm_units'],\n",
        "            hidden_layer_sizes=cfg['hidden_layer_sizes'],\n",
        "            dropout_rate=cfg['dropout_rate'],\n",
        "            weight_reg=cfg['weight_reg']\n",
        "        )\n",
        "\n",
        "        Optimizer = keras.optimizers.Adam if cfg['optimizer'] == 'adam' else keras.optimizers.RMSprop\n",
        "        model.compile(\n",
        "            loss={'review_score': 'mean_squared_error',\n",
        "                  'review_type':  'binary_crossentropy'},\n",
        "            loss_weights={'review_score': cfg['loss_weight_score'],\n",
        "                          'review_type':  cfg['loss_weight_type']},\n",
        "            optimizer=Optimizer(learning_rate=cfg['learning_rate']),\n",
        "            metrics={'review_score': ['mse', 'mae'],\n",
        "                     'review_type':  ['accuracy', 'binary_accuracy']}\n",
        "        )\n",
        "        print(\"Shapes: train_seqs:\", train_seqs.shape, \n",
        "              \"train_hot:\", train_hot.shape, \n",
        "              \"train_nums:\", train_nums.shape, \n",
        "              \"Review_Score:\", train_df['Review_Score'].shape, \n",
        "              \"Review_Type:\", train_df['Review_Type'].shape)\n",
        "\n",
        "        history = model.fit(\n",
        "            {'text_input': train_seqs,\n",
        "             'hotel_input': train_hot,\n",
        "             'numeric_input': train_nums},\n",
        "            {'review_score': train_df['Review_Score'],\n",
        "             'review_type':  train_df['Review_Type']},\n",
        "            epochs=cfg['epochs'],\n",
        "            batch_size=cfg['batch_size'],\n",
        "            validation_data=(\n",
        "                {'text_input': val_seqs,\n",
        "                 'hotel_input': val_hot,\n",
        "                 'numeric_input': val_nums},\n",
        "                {'review_score': val_df['Review_Score'],\n",
        "                 'review_type':  val_df['Review_Type']}\n",
        "            ),\n",
        "            verbose=1\n",
        "        )\n",
        "        print(\"Training history (last epoch):\")\n",
        "        for k, v in history.history.items():\n",
        "            print(f\"  {k}: {v[-1] if isinstance(v, list) else v}\")\n",
        "\n",
        "        pred_score, pred_type = model.predict(\n",
        "            {'text_input': val_seqs,\n",
        "             'hotel_input': val_hot,\n",
        "             'numeric_input': val_nums},\n",
        "            verbose=0\n",
        "        )\n",
        "        pred_label = (pred_type.flatten() >= 0.5).astype(int)\n",
        "        acc = accuracy_score(val_df['Review_Type'], pred_label)\n",
        "        f1  = f1_score(val_df['Review_Type'], pred_label)\n",
        "        mse = mean_squared_error(val_df['Review_Score'], pred_score.flatten())\n",
        "        merged = np.sqrt(acc * f1 * (1.0 / (np.log(mse + 1) + 1)))\n",
        "        print(f\"Evaluation metrics:\")\n",
        "        print(f\"  acc={acc:.4f}, f1={f1:.4f}, mse={mse:.4f}, merged={merged:.4f}\\n\")\n",
        "        if merged > best['overall'][0]:\n",
        "            best['overall'] = (merged, cfg)\n",
        "        if acc > best['classification'][0]:\n",
        "            best['classification'] = (acc, cfg)\n",
        "        if mse < best['regression'][0]:\n",
        "            best['regression'] = (mse, cfg)\n",
        "\n",
        "    print(\"\\n=== Best Results Summary ===\")\n",
        "    print(\"Best overall       :\", best['overall'])\n",
        "    print(\"Best classification:\", best['classification'])\n",
        "    print(\"Best regression    :\", best['regression'])\n",
        "    return best"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Evaluating Weighted Losses**\n",
        "It is crucial to note that when `loss_weights` are part of the hyperparameter search space, the total validation loss (`val_loss`) reported during training cannot be directly compared across different trials. Each combination of weights defines a different optimization objective.\n",
        "\n",
        "To ensure a fair and meaningful comparison, the selection of the best model is based on a **weight-independent evaluation metric**. My custom `merged` score is calculated on the raw prediction outputs (accuracy, F1-score, and MSE) of the validation set, regardless of the weights used to train the model. This approach decouples the training objective from the final performance assessment, allowing for an unbiased selection of the model that achieves the best practical trade-off between the two tasks.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_results = random_search_multitask(param_grid)\n",
        "print(\"Best overall   :\", best_results['overall'])\n",
        "print(\"Best class only:\", best_results['classification'])\n",
        "print(\"Best regress.  :\", best_results['regression'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As I have written during the exam, I'm going to run a random sample of configs from `param_grid`, <font color=\"red\">**CHANGE**</font> In this case, I also included the weight of each metric as part of the hyperparameter space. I had to make this change, because as you will see later in the code, the task of prediction of the class tends to prevail, and therefore it was necessary to test different combinations of the two weights. In this way it is also possible to trace the origin of the chosen weights.\n",
        "- For each:\n",
        "  - Compile with weighted MSE + BCE **losses** (used for tuning).\n",
        "  - Evaluate with a **custom merged metric**:  \n",
        "    $\\sqrt{\\text{accuracy} \\times \\text{F1} \\times \\left[\\frac{1}{\\log(\\text{MSE} + 1) + 1}\\right]}$\n",
        "  - Track best configs by:\n",
        "    - Overall (merged)\n",
        "    - Classification (accuracy)\n",
        "    - Regression (MSE)\n",
        "\n",
        "This formula was chosen for two reasons: 1) The geometric mean (using the square root) ensures that a model must perform well on all components to obtain a high score, heavily penalizing models that fail in a single task. 2) The logarithmic transformation of the MSE serves to normalize the regression error into a score similar to accuracy (the higher the better) and to dampen the impact of very large errors, making the metric more stable. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following configuration was identified as a well performing candidate during more extensive, offline experiments. For reproducibility within this notebook, we will use this specific configuration for final training and evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3Xv4zQbTTYA"
      },
      "outputs": [],
      "source": [
        "best_learning_rate = 0.01\n",
        "best_batch_size = 128\n",
        "best_epochs = 5\n",
        "best_dropout_rate = 0.1\n",
        "best_weight_reg = keras.regularizers.l2(1e-2) \n",
        "best_embedding_dim = 64\n",
        "best_lstm_units = 32\n",
        "best_optimizer = 'adam'\n",
        "best_hidden_layer_sizes = [64, 32]\n",
        "best_loss_weight_score = 5\n",
        "best_loss_weight_type = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "KL2fLtnT_9mC",
        "outputId": "42a51041-cacb-4cf9-e424-3b3f067bc2f5"
      },
      "outputs": [],
      "source": [
        "best_model = create_model(\n",
        "    text_vocab_size=VOCAB_SIZE,\n",
        "    hotel_onehot_dim=train_hot.shape[1],\n",
        "    numeric_features=train_nums.shape[1],\n",
        "    embedding_dim=best_embedding_dim,\n",
        "    lstm_units=best_lstm_units,\n",
        "    hidden_layer_sizes=best_hidden_layer_sizes,\n",
        "    dropout_rate=best_dropout_rate,\n",
        "    weight_reg=best_weight_reg\n",
        ")\n",
        "\n",
        "Optimizer = keras.optimizers.Adam if best_optimizer == 'adam' else keras.optimizers.RMSprop\n",
        "best_model.compile(\n",
        "    loss={'review_score': 'mean_squared_error',\n",
        "          'review_type':  'binary_crossentropy'},\n",
        "    loss_weights={'review_score': best_loss_weight_score,\n",
        "                  'review_type':  best_loss_weight_type},\n",
        "    optimizer=Optimizer(learning_rate=best_learning_rate),\n",
        "    metrics={'review_score': ['mse', 'mae'],\n",
        "             'review_type':  ['accuracy', 'binary_accuracy']}\n",
        ")\n",
        "\n",
        "history= best_model.fit(\n",
        "    {'text_input': train_seqs,\n",
        "     'hotel_input': train_hot,\n",
        "     'numeric_input': train_nums},\n",
        "    {'review_score': train_df['Review_Score'],\n",
        "     'review_type':  train_df['Review_Type']},\n",
        "    epochs=best_epochs,\n",
        "    batch_size=best_batch_size,\n",
        "    validation_data=(\n",
        "        {'text_input': val_seqs,\n",
        "         'hotel_input': val_hot,\n",
        "         'numeric_input': val_nums},\n",
        "        {'review_score': val_df['Review_Score'],\n",
        "         'review_type':  val_df['Review_Type']}\n",
        "    ),\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following plots visualize the model's performance during training for both the regression and classification tasks. We analyze the loss (MSE/BCE) and key metrics (MAE/Accuracy) for both training and validation sets. This allows us to assess model convergence, diagnose potential overfitting, and evaluate the effectiveness of our training strategy across epochs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_training_history(history):\n",
        "    \n",
        "    score_loss = 'review_score_loss'\n",
        "    val_score_loss = 'val_review_score_loss'\n",
        "    score_mae = 'review_score_mae'\n",
        "    val_score_mae = 'val_review_score_mae'\n",
        "\n",
        "    type_loss = 'review_type_loss'\n",
        "    val_type_loss = 'val_review_type_loss'\n",
        "    type_acc = 'review_type_accuracy'\n",
        "    val_type_acc = 'val_review_type_accuracy'\n",
        "\n",
        "    epochs = range(1, len(history.history[score_loss]) + 1)\n",
        "\n",
        "    fig, axs = plt.subplots(2, 2, figsize=(18, 12))\n",
        "    fig.suptitle('Training and Validation History', fontsize=20)\n",
        "\n",
        "    axs[0, 0].plot(epochs, history.history[score_loss], 'bo-', label='Training Score Loss (MSE)')\n",
        "    axs[0, 0].plot(epochs, history.history[val_score_loss], 'ro-', label='Validation Score Loss (MSE)')\n",
        "    axs[0, 0].set_title('Regression Task: Score Loss (MSE)')\n",
        "    axs[0, 0].set_xlabel('Epochs')\n",
        "    axs[0, 0].set_ylabel('Loss (MSE)')\n",
        "    axs[0, 0].legend()\n",
        "    axs[0, 0].grid(True)\n",
        "    \n",
        "    axs[0, 1].plot(epochs, history.history[score_mae], 'bo-', label='Training Score MAE')\n",
        "    axs[0, 1].plot(epochs, history.history[val_score_mae], 'ro-', label='Validation Score MAE')\n",
        "    axs[0, 1].set_title('Regression Task: Score Mean Absolute Error')\n",
        "    axs[0, 1].set_xlabel('Epochs')\n",
        "    axs[0, 1].set_ylabel('MAE')\n",
        "    axs[0, 1].legend()\n",
        "    axs[0, 1].grid(True)\n",
        "\n",
        "    axs[1, 0].plot(epochs, history.history[type_loss], 'bo-', label='Training Type Loss (BCE)')\n",
        "    axs[1, 0].plot(epochs, history.history[val_type_loss], 'ro-', label='Validation Type Loss (BCE)')\n",
        "    axs[1, 0].set_title('Classification Task: Type Loss (Binary Crossentropy)')\n",
        "    axs[1, 0].set_xlabel('Epochs')\n",
        "    axs[1, 0].set_ylabel('Loss (BCE)')\n",
        "    axs[1, 0].legend()\n",
        "    axs[1, 0].grid(True)\n",
        "\n",
        "    axs[1, 1].plot(epochs, history.history[type_acc], 'bo-', label='Training Type Accuracy')\n",
        "    axs[1, 1].plot(epochs, history.history[val_type_acc], 'ro-', label='Validation Type Accuracy')\n",
        "    axs[1, 1].set_title('Classification Task: Type Accuracy')\n",
        "    axs[1, 1].set_xlabel('Epochs')\n",
        "    axs[1, 1].set_ylabel('Accuracy')\n",
        "    axs[1, 1].legend()\n",
        "    axs[1, 1].grid(True)\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "    plt.show()\n",
        "\n",
        "plot_training_history(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The graphs show that the model converges quickly but begins to show clear overfitting after the first few epochs.\n",
        "Regression Task: The validation metrics (MSE and MAE, in red) stabilize after epoch 2, while the training metrics continue to improve, indicating that the model is memorizing the training data.\n",
        "Classification Task: Validation accuracy (bottom right) peaks at epoch 4 and then declines, a classic sign that the model has exceeded its optimal point of generalization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 6 - MODEL EVALUATION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This section is dedicated to the final evaluation of the best-performing model, identified through the hyperparameter optimization process. As stated in the exam, the evaluation addresses the model's multi-task nature by assessing its performance on both the **regression task** (predicting the `Review Score`) and the **classification task** (predicting the `Review Type`).\n",
        "\n",
        "The evaluation is conducted on the **unseen test set** to provide an unbiased estimate of the model's generalization capabilities. The process follows the plan outlined in the exam:\n",
        "1.  Evaluate each task separately using appropriate metrics (MSE for regression, Accuracy/F1 for classification).\n",
        "2.  Visualize the results to gain deeper insights (e.g., Confusion Matrix and performance plots).\n",
        "3.  Calculate the custom **merged evaluation score** to provide a single, complete measure of performance, as proposed in my exam."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_pred_score, test_pred_type = best_model.predict(\n",
        "    {'text_input': test_seqs,\n",
        "     'hotel_input': test_hot,\n",
        "     'numeric_input': test_nums},\n",
        "    verbose=0\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.1. Clipping \n",
        "As said during the exam, as a post-processing step, i'm going to clip the regression outputs to the valid score range [2.5, 10.0], This ensures that the model does not produce physically impossible scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "min_score, max_score = 2.5, 10.0\n",
        "test_pred_score_clipped = np.clip(test_pred_score.flatten(), min_score, max_score)\n",
        "test_pred_label = (test_pred_type.flatten() >= 0.5).astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2 Performance Visualization\n",
        "To immediately see the model's combined performance, the following scatterplot visualizes the relationship between the true and predicted scores, while also showing the true class of each review. This plot provides a rich, multi-faceted view of the model's behavior. An ideal model would place all points along the dashed \"Perfect Prediction\" line.The color coding reveals how prediction errors are distributed across the two classes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "legend_labels = {0: 'Bad Review', 1: 'Good Review'}\n",
        "plt.figure(figsize=(10, 10))\n",
        "sns.scatterplot(x=test_df['Review_Score'], y=test_pred_score_clipped, hue=test_df['Review_Type'],\n",
        "                palette=['#E74C3C', '#2ECC71'], alpha=0.6, s=50)\n",
        "plt.plot([min_score, max_score], [min_score, max_score], 'k--', lw=2, label='Perfect Prediction')\n",
        "plt.xlabel('True Score', fontsize=12)\n",
        "plt.ylabel('Predicted Score', fontsize=12)\n",
        "plt.title('True vs. Predicted Score (Colored by True Review Type)', fontsize=16)\n",
        "plt.legend(title='True Type', labels=[legend_labels[i] for i in sorted(test_df['Review_Type'].unique())])\n",
        "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analysis of the Performance Plot\n",
        "\n",
        "The scatterplot provides a crucial insight into the model's behavior within its multi-task learning framework. A detailed analysis reveals an intresting dynamic:\n",
        "\n",
        "1.  **Excellent Classification Performance:** The model demonstrates a remarkable ability to distinguish between \"Good Reviews\" and \"Bad Reviews\", as shown by the clear separation of colors. This indicates that the classification head of the model has learned very strong patterns from the data.\n",
        "\n",
        "2.  **Regression:** For the regression task, the model appears to have adopted a heuristic-based approach rather than a fine-grained prediction model. It has correctly identified the strong correlation between the review type and the score range. Consequently:\n",
        "    *   When a review is classified as \"Bad,\" the model predicts a score within a general \"low-to-medium\" range.\n",
        "    *   When a review is classified as \"Good,\" it predicts a score within a \"high\" range.\n",
        "\n",
        "**Why this happens?**\n",
        "\n",
        "I think that the classification task, likely being \"easier\" or having a steeper loss gradient, has dominated the training process. By mastering the classification, the model already achieves a reasonably low Mean Squared Error, thus reducing the learning incentive to further refine the exact score prediction.\n",
        "\n",
        "**Addressing the Imbalance**\n",
        "To validate this hypothesis, I experimented by significantly increasing the weight of the regression loss (`loss_weight_score`) during training. This forces the model to prioritize the reduction of the MSE. This effectively led to some improvements. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.3 Task-Specific Quantitative Evaluation\n",
        "Following the visual analysis, we now compute the specific metrics for each task as planned in the exam."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nRegression Task\")\n",
        "test_mse = mean_squared_error(test_df['Review_Score'], test_pred_score_clipped)\n",
        "print(f\"Mean Squared Error on Test Set: {test_mse:.4f}\")\n",
        "\n",
        "def evaluate_score_on_scale(true_scores, predicted_scores, tolerance=1.5):\n",
        "    correct_within_tolerance = np.sum(np.abs(true_scores - predicted_scores) <= tolerance)\n",
        "    accuracy_within_tolerance = correct_within_tolerance / len(true_scores)\n",
        "    print(f\"Accuracy within +/- {tolerance} score points: {accuracy_within_tolerance:.2%}\")\n",
        "\n",
        "evaluate_score_on_scale(test_df['Review_Score'].values, test_pred_score_clipped)\n",
        "\n",
        "print(\"\\nClassification Task\")\n",
        "test_accuracy = accuracy_score(test_df['Review_Type'], test_pred_label)\n",
        "test_f1 = f1_score(test_df['Review_Type'], test_pred_label)\n",
        "\n",
        "print(f\"Accuracy on Test Set: {test_accuracy:.4f}\")\n",
        "print(f\"F1 Score on Test Set: {test_f1:.4f}\")\n",
        "\n",
        "cm = confusion_matrix(test_df['Review_Type'], test_pred_label)\n",
        "plt.figure(figsize=(7, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Bad Review (0)', 'Good Review (1)'],\n",
        "            yticklabels=['Bad Review (0)', 'Good Review (1)'])\n",
        "plt.xlabel('Predicted Label', fontsize=12)\n",
        "plt.ylabel('True Label', fontsize=12)\n",
        "plt.title('Confusion Matrix for Review Type Prediction', fontsize=16)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.4 Merged Evaluation Score\n",
        "Finally, as proposed in the exam, I compute the single merged score that combines the performance on both tasks. This metric, already used during hyperparameter tuning, provides a unified view of the model's overall quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_test_score = np.sqrt(test_accuracy * test_f1 * (1.0 / (np.log(test_mse + 1) + 1)))\n",
        "print(f\"Final Merged Evaluation Score on Test Set: {merged_test_score:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
